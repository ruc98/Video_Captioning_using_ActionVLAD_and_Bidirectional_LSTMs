# -*- coding: utf-8 -*-
"""Untitled1 (1).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ILFYQJXwsgP7b7QAOfLFKkgaujfeAOuy
"""

from google.colab import drive
drive.mount('/gdrive')

!cp /gdrive/'My Drive'/4_data.zip data1.zip
!unzip data1.zip

!python3 train.py

!kill -9 -1

!cp /gdrive/'My Drive'/colab/npcap1.npy npcap1.npy

import numpy as np
import os

import torch
import torch.utils.data as data_utils
from torch import nn
from torch.autograd import Variable
from torch.utils.data import DataLoader
from torchvision import transforms
import torch.nn.functional as F
import torch.optim as optim

torch.set_default_tensor_type('torch.cuda.FloatTensor')

def prepare_sequence(seq, to_ix):
    idxs = []
    for w in seq:
        if w not in to_ix:
            idxs.append(0)
        else:
            idxs.append(to_ix[w])
    return torch.tensor(idxs, dtype=torch.long).cuda()

num_clus=3
beta=0.0001
#vl=np.zeros(shape=(100,3,10)) #(n,k,D)


word_to_ix = {}
s=None
cap_train = None

with open('7.txt') as f:
  s=f.readlines() #list of sentences (num_examples)
  cap_train=[[] for i in range(int(len(s)))]
  for idx,i in enumerate(s):
    #print(i)
    #print(i.split('|'))
    #print(i.split('|')[0])
    te=i.split('|')[2].split()
    
    for j in te:
      j.lower()
      j.strip('\n')
      j=j.replace('.','')
      #print(j)
      if (j != ''):
        cap_train[idx].append(j)
      if j not in word_to_ix and j != '.' and j != '':
        word_to_ix[j]=len(word_to_ix)
print(len(word_to_ix))

class DecoderRNN(nn.Module):
    def __init__(self, embed_size, hidden_size, vocab_size, num_layers=1):
        super(DecoderRNN, self).__init__()
        
        # define the properties
        self.embed_size = embed_size
        self.hidden_size = hidden_size
        self.vocab_size = vocab_size
        
        # lstm cell
        self.lstm_cell = nn.LSTMCell(input_size=embed_size, hidden_size=hidden_size)
    
        # output fully connected layer
        self.fc_out = nn.Linear(in_features=self.hidden_size, out_features=self.vocab_size)
    
        # embedding layer
        self.embed = nn.Embedding(num_embeddings=self.vocab_size, embedding_dim=self.embed_size)
    
        # activations
        self.softmax = nn.Softmax(dim=1)
    
    def forward(self, captions, features):
        features = torch.tensor(features[np.newaxis,:],dtype=torch.float32)
        
        # batch size
        batch_size = 1
        
        # init the hidden and cell states to zeros
        hidden_state = torch.zeros((batch_size, self.hidden_size))
        cell_state = torch.zeros((batch_size, self.hidden_size))
    
        # define the output tensor placeholder
        outputs = torch.empty((batch_size, captions.size(0), self.vocab_size))
        
        

        # embed the captions
        captions_embed = self.embed(captions)
        #print(captions_embed.shape)
        
        # pass the caption word by word
        for t in range(captions.size(0)):
        
            # for the first time step the input is the feature vector
            if t == 0:
                #print(features.shape)
                hidden_state, cell_state = self.lstm_cell(features, (hidden_state, cell_state))
                
            # for the 2nd+ time step, using teacher forcer
            else:
                #print(t-1)
                #print(captions_embed[t-1,:].shape)
                #tep=captions_embed[t-1,:]
                #print(tep.shape)
                #print(t-1,captions_embed.shape)
                #tep = torch.tensor(tep[np.newaxis,:],dtype=torch.float32)
                hidden_state, cell_state = self.lstm_cell(captions_embed[t-1, :].view(1,-1), (hidden_state, cell_state))
            
            # output of the attention mechanism
            out = self.fc_out(hidden_state)
            
            # build the output tensor
            outputs[:,t, :] = out
    
        return outputs

features = np.zeros(shape=(vlad.shape[0],vlad.shape[1]*vlad.shape[2])) # N*K*D
for i in range(vlad.shape[0]):
  ij=0
  for j in range(vlad.shape[1]):
    for k in range(vlad.shape[2]):
      features[i][ij]=vlad[i][j][k]
      ij=ij+1
    
EMBEDDING_DIM = 8192
HIDDEN_DIM = 40
print(features.shape)
vocab_size=len(word_to_ix)

HIDDEN_DIM = 40
EMBEDDING_DIM = 3072

vlad=np.load('npcap1.npy')

model = DecoderRNN(EMBEDDING_DIM, HIDDEN_DIM, vocab_size, num_layers=1)
model.cuda()
loss_function = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.002)

er=0
pr=0
prev_loss=100000

for epoch in range(30):  # again, normally you would NOT do 300 epochs, it is toy data
    loss_epoch=0
    for i in range(1400):
        for j in range(5):
            #print(i)
            sentence = cap_train[5*i+j]
            model.zero_grad()

            cap_target = prepare_sequence(sentence, word_to_ix)
            
            cap_pred = model(cap_target,features[i])
            #print(" ".join(sentence))
            #print(calc(cap_pred))
            try:
              loss = loss_function(cap_pred.view(-1,vocab_size), cap_target.contiguous().view(-1))
              #print('h')
              loss.backward()
              #print('h1')
              loss_epoch+=loss.data.item()
              #print('h2')
            except:
              er=er+1
              pass
            optimizer.step()
            if(er > pr):
              pr=er
              print('error')
              print(i,j,er)
        #print(loss.data.item())
    #torch.save(model.state_dict(), './sim_vgg.pth')
    print(loss_epoch)
        
    prev_loss=loss_epoch

er=0
pr=0

hypothesis=[]
reference=[]
for epoch in range(1):  # again, normally you would NOT do 300 epochs, it is toy data
    loss_epoch=0
    to=0
    for i in range(70,100,1):
        ma=0
        for j in range(5):
            sentence = cap_train[5*i+j]
            model.zero_grad()
            #reference.append(" ".join(sentence))
            cap_target = prepare_sequence(sentence, word_to_ix)

            cap_pred = model(cap_target,features[i])
            cap_pred = cap_pred.view(-1,vocab_size)
            maxval, maxidx = torch.max(cap_pred,dim=-1)
            sent=[]
            key_list=list(word_to_ix.keys())    
            for k in range(maxidx.shape[0]):
                sent.append(key_list[maxidx[k]])

            #hypothesis.append(" ".join(sent))
            #print(" ".join(sent))
            #print(" ".join(sentence))
            te=get_moses_multi_bleu(" ".join(sent), " ".join(sentence), lowercase=True)
            #print(te)
            if(te > ma):
              ma=te
        to=to+ma
        #print(ma)
    to=to/30
    print(to)
              
#print(get_moses_multi_bleu(hypothesis, reference, lowercase=True))

er=0
pr=0

hypothesis=[]
reference=[]
for epoch in range(1):  # again, normally you would NOT do 300 epochs, it is toy data
    loss_epoch=0
    for i in range(1400,2000,1):
        for j in range(5):
            sentence = cap_train[5*i+j]
            model.zero_grad()
            reference.append(" ".join(sentence))
            cap_target = prepare_sequence(sentence, word_to_ix)

            cap_pred = model(cap_target,features[i])
            cap_pred = cap_pred.view(-1,vocab_size)
            maxval, maxidx = torch.max(cap_pred,dim=-1)
            sent=[]
            key_list=list(word_to_ix.keys())    
            for k in range(maxidx.shape[0]):
                sent.append(key_list[maxidx[k]])

            hypothesis.append(" ".join(sent))
print(get_moses_multi_bleu(hypothesis, reference, lowercase=True))

er=0
pr=0

hypothesis=[]
reference=[]
for epoch in range(1):  # again, normally you would NOT do 300 epochs, it is toy data
    loss_epoch=0
    for i in range(0,1400,1):
        for j in range(5):
            sentence = cap_train[5*i+j]
            model.zero_grad()
            reference.append(" ".join(sentence))
            cap_target = prepare_sequence(sentence, word_to_ix)

            cap_pred = model(cap_target,features[i])
            cap_pred = cap_pred.view(-1,vocab_size)
            maxval, maxidx = torch.max(cap_pred,dim=-1)
            sent=[]
            key_list=list(word_to_ix.keys())    
            for k in range(maxidx.shape[0]):
                sent.append(key_list[maxidx[k]])

            hypothesis.append(" ".join(sent))
print(get_moses_multi_bleu(hypothesis, reference, lowercase=True))

HIDDEN_DIM = 20

model1 = DecoderRNN(EMBEDDING_DIM, HIDDEN_DIM, vocab_size, num_layers=1)
model1.cuda()
loss_function = nn.CrossEntropyLoss()
optimizer = optim.Adam(model1.parameters(), lr=0.001)

er=0
pr=0
prev_loss=100000

for epoch in range(30):  # again, normally you would NOT do 300 epochs, it is toy data
    loss_epoch=0
    for i in range(1400):
        for j in range(5):
            #print(i)
            sentence = cap_train[5*i+j]
            model1.zero_grad()

            cap_target = prepare_sequence(sentence, word_to_ix)
            
            cap_pred = model1(cap_target,features[i])
            #print(" ".join(sentence))
            #print(calc(cap_pred))
            try:
              loss = loss_function(cap_pred.view(-1,vocab_size), cap_target.contiguous().view(-1))
              #print('h')
              loss.backward()
              #print('h1')
              loss_epoch+=loss.data.item()
              #print('h2')
            except:
              er=er+1
              pass
            optimizer.step()
            if(er > pr):
              pr=er
              print('error')
              print(i,j,er)
        #print(loss.data.item())
    #torch.save(model1.state_dict(), './sim_vgg.pth')
    print(loss_epoch)
    
    prev_loss=loss_epoch

er=0
pr=0

hypothesis=[]
reference=[]
for epoch in range(1):  # again, normally you would NOT do 300 epochs, it is toy data
    loss_epoch=0
    for i in range(0,1400,1):
        for j in range(5):
            sentence = cap_train[5*i+j]
            model1.zero_grad()
            reference.append(" ".join(sentence))
            cap_target = prepare_sequence(sentence, word_to_ix)

            cap_pred = model1(cap_target,features[i])
            cap_pred = cap_pred.view(-1,vocab_size)
            maxval, maxidx = torch.max(cap_pred,dim=-1)
            sent=[]
            key_list=list(word_to_ix.keys())    
            for k in range(maxidx.shape[0]):
                sent.append(key_list[maxidx[k]])

            hypothesis.append(" ".join(sent))
print(get_moses_multi_bleu(hypothesis, reference, lowercase=True))

er=0
pr=0

hypothesis=[]
reference=[]
for epoch in range(1):  # again, normally you would NOT do 300 epochs, it is toy data
    loss_epoch=0
    for i in range(1400,2000,1):
        for j in range(5):
            sentence = cap_train[5*i+j]
            model1.zero_grad()
            reference.append(" ".join(sentence))
            cap_target = prepare_sequence(sentence, word_to_ix)

            cap_pred = model1(cap_target,features[i])
            cap_pred = cap_pred.view(-1,vocab_size)
            maxval, maxidx = torch.max(cap_pred,dim=-1)
            sent=[]
            key_list=list(word_to_ix.keys())    
            for k in range(maxidx.shape[0]):
                sent.append(key_list[maxidx[k]])

            hypothesis.append(" ".join(sent))
print(get_moses_multi_bleu(hypothesis, reference, lowercase=True))

HIDDEN_DIM = 10

model2 = DecoderRNN(EMBEDDING_DIM, HIDDEN_DIM, vocab_size, num_layers=1)
model2.cuda()
loss_function = nn.CrossEntropyLoss()
optimizer = optim.Adam(model2.parameters(), lr=0.001)

er=0
pr=0
prev_loss=100000

for epoch in range(30):  # again, normally you would NOT do 300 epochs, it is toy data
    loss_epoch=0
    for i in range(1400):
        for j in range(5):
            #print(i)
            sentence = cap_train[5*i+j]
            model2.zero_grad()

            cap_target = prepare_sequence(sentence, word_to_ix)
            
            cap_pred = model2(cap_target,features[i])
            #print(" ".join(sentence))
            #print(calc(cap_pred))
            try:
              loss = loss_function(cap_pred.view(-1,vocab_size), cap_target.contiguous().view(-1))
              #print('h')
              loss.backward()
              #print('h1')
              loss_epoch+=loss.data.item()
              #print('h2')
            except:
              er=er+1
              pass
            optimizer.step()
            if(er > pr):
              pr=er
              print('error')
              print(i,j,er)
        #print(loss.data.item())
    #torch.save(model2.state_dict(), './sim_vgg.pth')
    print(loss_epoch)
    
    prev_loss=loss_epoch

er=0
pr=0

hypothesis=[]
reference=[]
for epoch in range(1):  # again, normally you would NOT do 300 epochs, it is toy data
    loss_epoch=0
    for i in range(0,1400,1):
        for j in range(5):
            sentence = cap_train[5*i+j]
            model2.zero_grad()
            reference.append(" ".join(sentence))
            cap_target = prepare_sequence(sentence, word_to_ix)

            cap_pred = model2(cap_target,features[i])
            cap_pred = cap_pred.view(-1,vocab_size)
            maxval, maxidx = torch.max(cap_pred,dim=-1)
            sent=[]
            key_list=list(word_to_ix.keys())    
            for k in range(maxidx.shape[0]):
                sent.append(key_list[maxidx[k]])

            hypothesis.append(" ".join(sent))
print(get_moses_multi_bleu(hypothesis, reference, lowercase=True))

er=0
pr=0

hypothesis=[]
reference=[]
for epoch in range(1):  # again, normally you would NOT do 300 epochs, it is toy data
    loss_epoch=0
    for i in range(1400,2000,1):
        for j in range(5):
            sentence = cap_train[5*i+j]
            model2.zero_grad()
            reference.append(" ".join(sentence))
            cap_target = prepare_sequence(sentence, word_to_ix)

            cap_pred = model2(cap_target,features[i])
            cap_pred = cap_pred.view(-1,vocab_size)
            maxval, maxidx = torch.max(cap_pred,dim=-1)
            sent=[]
            key_list=list(word_to_ix.keys())    
            for k in range(maxidx.shape[0]):
                sent.append(key_list[maxidx[k]])

            hypothesis.append(" ".join(sent))
print(get_moses_multi_bleu(hypothesis, reference, lowercase=True))

from torchnlp.metrics import get_moses_multi_bleu

def calc(cap_pred):
  #print(cap_pred)
  sentence=[]
  key_list=list(word_to_ix.keys())
  for i in range(cap_pred.shape[0]):
    for j in range(cap_pred.shape[1]):
      max=-100000000000000
      index=-1
      for k in range(cap_pred.shape[2]):
        if cap_pred[i][j][k].item()>max:
          max=cap_pred[i][j][k].item()
          index=k
      #print(max,index)
      sentence.append(key_list[index])
  return " ".join(sentence)

!pip install pytorch-nlp

!zip -r 4_data.zip /content/Image_Captioning/7

import csv 
import torch
import math
import numpy as np
import torch.nn as nn
from sklearn.cluster import KMeans
import torchvision.models as models
import matplotlib.image as img
import PIL.Image
  
#googlenet model
# csv file name 
filename = "ImageID.csv"
  
# initializing the titles and rows list 
fields = [] 
rows = [] 
imdata=None
model = googlenet(pretrained=True)
#new_classifier = nn.Sequential(*list(model.classifier.children())[:-7])
#model.classifier = new_classifier
#model.children=nn.Sequential(*list(model.children())[:1])
#print(model.layers)
#print(*list(model.features))

image=np.zeros(shape=(1,224,224,3))
image=np.rollaxis(image,3,1)
a=torch.tensor(image,dtype=torch.float32)
print(model(a).shape)
dim=model(a).shape[3]
dim1=model(a).shape[1]
print(dim)

num_clus=8
vlad=None
beta=0.0001

with open('7.txt') as f:
  s=f.readlines()
  x=np.zeros(shape=(dim*dim,dim1))
  vlad=np.zeros(shape=(int(len(s)/5),num_clus,dim1))
  for i in range(0,len(s),5):
    print(i)
    #print(s[i])
    try:
      image=np.zeros(shape=(1,224,224,3))
      ty=s[i].split('|')[0]
      image[0] = img.imread('content/Image_Captioning/7/train_7/'+ty)
      #print(image.shape)
      image=np.rollaxis(image,3,1)
      a=torch.tensor(image,dtype=torch.float32)
      imdata=model(a).detach().numpy()
      #print(imdata)

      ij=0
      for h in  range(dim):
        for j in range(dim):
          x[ij]=imdata[0,:,h,j]
          ij=ij+1
      kmeans = KMeans(n_clusters=num_clus, random_state=0).fit(x)
      #print(kmeans.cluster_centers_)
      for h in range(num_clus):
        te=np.zeros(shape=(dim1))
        to=0
        a=np.zeros(shape=(dim*dim))
        for j in range(dim*dim):
          #print(np.dot(x[j]-kmeans.cluster_centers_[h],x[j]-kmeans.cluster_centers_[h]))
          a[j]=math.exp(-(beta*np.dot(x[j]-kmeans.cluster_centers_[h],x[j]-kmeans.cluster_centers_[h])))
          to=to+a[j]
          #print(a[j])
        a=a/to
        #print(to)
        for j in range(dim*dim):
          te=te+a[j]*(x[j]-kmeans.cluster_centers_[h])
        vlad[int(i/5)][h]=te
    except:
      pass
      #print(te)

np.save('npcap1_8',vlad)

!cp /content/npcap1_8.npy /gdrive/'My Drive'

import csv 
import torch
import math
import numpy as np
import torch.nn as nn
from sklearn.cluster import KMeans
import torchvision.models as models
import matplotlib.image as img
import PIL.Image
  
# csv file name 
filename = "ImageID.csv"
  
# initializing the titles and rows list 
fields = [] 
rows = [] 
imdata=None
model = models.vgg16(pretrained=True)
#new_classifier = nn.Sequential(*list(model.classifier.children())[:-7])
#model.classifier = new_classifier
#model.children=nn.Sequential(*list(model.children())[:1])
#print(model.layers)
#print(*list(model.features))

image=np.zeros(shape=(1,224,224,3))
image=np.rollaxis(image,3,1)
a=torch.tensor(image,dtype=torch.float32)
print(model.features(a).shape)
dim=model.features(a).shape[3]
dim1=model.features(a).shape[1]
print(dim)

num_clus=3
vlad=None
beta=0.0001

with open('7.txt') as f:
  s=f.readlines()
  x=np.zeros(shape=(dim*dim,dim1))
  vlad=np.zeros(shape=(int(len(s)/5),num_clus,dim1))
  for i in range(0,len(s),5):
    print(i)
    #print(s[i])
    image=np.zeros(shape=(1,224,224,3))
    ty=s[i].split('|')[0]
    image[0] = img.imread('content/Image_Captioning/7/train_7/'+ty)
    #print(image.shape)
    image=np.rollaxis(image,3,1)
    a=torch.tensor(image,dtype=torch.float32)
    imdata=model.features(a).detach().numpy()
    #print(imdata)
    
    ij=0
    for h in  range(dim):
      for j in range(dim):
        x[ij]=imdata[0,:,h,j]
        ij=ij+1
    kmeans = KMeans(n_clusters=num_clus, random_state=0).fit(x)
    #print(kmeans.cluster_centers_)
    for h in range(num_clus):
      te=np.zeros(shape=(dim1))
      to=0
      a=np.zeros(shape=(dim*dim))
      for j in range(dim*dim):
        #print(np.dot(x[j]-kmeans.cluster_centers_[h],x[j]-kmeans.cluster_centers_[h]))
        a[j]=math.exp(-(beta*np.dot(x[j]-kmeans.cluster_centers_[h],x[j]-kmeans.cluster_centers_[h])))
        to=to+a[j]
        #print(a[j])
      a=a/to
      #print(to)
      for j in range(dim*dim):
        te=te+a[j]*(x[j]-kmeans.cluster_centers_[h])
      vlad[int(i/5)][h]=te
      #print(te)

import csv 
import torch
import math
import numpy as np
import torch.nn as nn
from sklearn.cluster import KMeans
import torchvision.models as models
import matplotlib.image as img
import PIL.Image
  
# csv file name 
filename = "ImageID.csv"
  
# initializing the titles and rows list 
fields = [] 
rows = [] 
imdata=None
model = models.vgg16(pretrained=True)
#new_classifier = nn.Sequential(*list(model.classifier.children())[:-7])
#model.classifier = new_classifier
#model.children=nn.Sequential(*list(model.children())[:1])
#print(model.layers)
#print(*list(model.features))

image=np.zeros(shape=(1,224,224,3))
image=np.rollaxis(image,3,1)
a=torch.tensor(image,dtype=torch.float32)
print(model.features(a).shape)
dim=model.features(a).shape[3]
dim1=model.features(a).shape[1]
print(dim)

num_clus=3
vlad=None
beta=0.0001

with open('7.txt') as f:
  s=f.readlines()
  x=np.zeros(shape=(dim*dim,dim1))
  vlad=np.zeros(shape=(int(len(s)/5),num_clus,dim1))
  a=np.zeros(shape=(dim*dim))
  for j in range(int(len(s)/600),len(s),int(len(s)/600)):
    image=np.zeros(shape=(int(len(s)/600),224,224,3))
    for i in range(max(0,j-int(len(s)/600)),j,5):
      print(i)
      #print(s[i])
      ty=s[i].split('|')[0]
      image[int(i/5)%int(len(s)/600)] = img.imread('content/Image_Captioning/7/train_7/'+ty)
      #print(image.shape)
    image=np.rollaxis(image,3,1)
    print('hi')
    gh=torch.tensor(image,dtype=torch.float32)
    print('bi')
    imdata=model.features(gh).detach().numpy()
    print('ki')
    #print(imdata)
    
    for i in range(max(0,j-int(len(s)/600)),j,5):
      ij=0
      for h in  range(dim):
        for j in range(dim):
          x[ij]=imdata[int(i/5)%int(len(s)/600),:,h,j]
          ij=ij+1
      kmeans = KMeans(n_clusters=num_clus, random_state=0).fit(x)
      #print(kmeans.cluster_centers_)
      for h in range(num_clus):
        te=np.zeros(shape=(dim1))
        to=0
        for j in range(dim*dim):
          #print(np.dot(x[j]-kmeans.cluster_centers_[h],x[j]-kmeans.cluster_centers_[h]))
          a[j]=math.exp(-(beta*np.dot(x[j]-kmeans.cluster_centers_[h],x[j]-kmeans.cluster_centers_[h])))
          to=to+a[j]
          #print(a[j])
        a=a/to
        #print(to)
        for j in range(dim*dim):
          te=te+a[j]*(x[j]-kmeans.cluster_centers_[h])
        vlad[int(i/5)][h]=te
        #print(te)

dic={}
with open('7.txt') as f:
  s=f.readlines()
  for i in s:
    #print(i)
    #print(i.split('|'))
    #print(i.split('|')[0])
    te=i.split('|')[2].split()
    for j in te:
      j.lower()
      j.strip('\n')
      dic[j]=1

li=list(dic.keys())
print(len(li))
print(li)
dic={}
for i in range(len(li)):
  dic[li[i]]=i

from PIL import Image

import os
rootdir = '/content/Image_Captioning/7/train_7'

for subdir, dirs, files in os.walk(rootdir):
	for file in files:
		foo = Image.open(os.path.join(subdir, file))
		foo = foo.resize((224,224),Image.ANTIALIAS)
		foo.save(os.path.join(subdir, file),quality=95)

import csv 
import torch
import math
import numpy as np
import torch.nn as nn
from sklearn.cluster import KMeans
import torchvision.models as models
import matplotlib.image as img
import PIL.Image
  
# csv file name 
filename = "ImageID.csv"
  
# initializing the titles and rows list 
fields = [] 
rows = [] 
imdata=None
model = models.vgg16(pretrained=True)
#new_classifier = nn.Sequential(*list(model.classifier.children())[:-7])
#model.classifier = new_classifier
#model.children=nn.Sequential(*list(model.children())[:1])
#print(model.layers)
#print(*list(model.features))

image=np.zeros(shape=(1,224,224,3))
image=np.rollaxis(image,3,1)
a=torch.tensor(image,dtype=torch.float32)
print(model.features(a).shape)
dim=model.features(a).shape[3]
dim1=model.features(a).shape[1]
model=None
print(dim)

def prepare_sequence(seq, to_ix):
  idxs = [to_ix[w] for w in seq]
  return torch.tensor(idxs, dtype=torch.long)

num_clus=3
beta=0.0001
#vl=np.zeros(shape=(100,3,10)) #(n,k,D)

class DecoderRNN(nn.Module):
    def __init__(self, embed_size, hidden_size, vocab_size, num_layers=1):
        super(DecoderRNN, self).__init__()
        
        # define the properties
        self.embed_size = embed_size
        self.hidden_size = hidden_size
        self.vocab_size = vocab_size
        
        # lstm cell
        self.lstm_cell = nn.LSTMCell(input_size=embed_size, hidden_size=hidden_size)
    
        # output fully connected layer
        self.fc_out = nn.Linear(in_features=self.hidden_size, out_features=self.vocab_size)
    
        # embedding layer
        self.embed = nn.Embedding(num_embeddings=self.vocab_size, embedding_dim=self.embed_size)
    
        # activations
        self.softmax = nn.Softmax(dim=1)
    
    def forward(self, features, captions):
        
        # batch size
        batch_size = features.size(0)
        
        # init the hidden and cell states to zeros
        hidden_state = torch.zeros((batch_size, self.hidden_size)).cuda()
        cell_state = torch.zeros((batch_size, self.hidden_size)).cuda()
    
        # define the output tensor placeholder
        outputs = torch.empty((batch_size, captions.size(1), self.vocab_size)).cuda()

        # embed the captions
        captions_embed = self.embed(captions)
        
        # pass the caption word by word
        for t in range(captions.size(1)):

            # for the first time step the input is the feature vector
            if t == 0:
                hidden_state, cell_state = self.lstm_cell(features, (hidden_state, cell_state))
                
            # for the 2nd+ time step, using teacher forcer
            else:
                hidden_state, cell_state = self.lstm_cell(features, (hidden_state, cell_state))
            
            # output of the attention mechanism
            out = self.fc_out(hidden_state)
            
            # build the output tensor
            outputs[:, t, :] = out
    
        return outputs
  
word_to_ix = {}
s=None # s[0]=['the','rthd','dfhd']
vlad=None
      
with open('7.txt') as f:
  s=f.readlines() #list of sentences (num_examples)
  vlad=np.zeros(shape=(int(len(s)/5),num_clus,dim1))
  for i in s:
    #print(i)
    #print(i.split('|'))
    #print(i.split('|')[0])
    te=i.split('|')[2].split()
    for j in te:
      j.lower()
      j.strip('\n')
      if j not in word_to_ix:
        word_to_ix[j]=len(word_to_ix)
print(len(word_to_ix))

for i in range(vlad.shape[0]):
  cap=[[] for i in range(int(len(s)/5))]

import csv 
import torch
import numpy as np
import torch.nn as nn
import torchvision.models as models
import matplotlib.image as img
import PIL.Image
import warnings
from collections import namedtuple
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils import model_zoo

__all__ = ['GoogLeNet', 'googlenet']

model_urls = {
    # GoogLeNet ported from TensorFlow
    'googlenet': 'https://download.pytorch.org/models/googlenet-1378be20.pth',
}

_GoogLeNetOuputs = namedtuple('GoogLeNetOuputs', ['logits', 'aux_logits2', 'aux_logits1'])


def googlenet(pretrained=False, **kwargs):
    r"""GoogLeNet (Inception v1) model architecture from
    `"Going Deeper with Convolutions" <http://arxiv.org/abs/1409.4842>`_.
    Args:
        pretrained (bool): If True, returns a model pre-trained on ImageNet
        aux_logits (bool): If True, adds two auxiliary branches that can improve training.
            Default: *False* when pretrained is True otherwise *True*
        transform_input (bool): If True, preprocesses the input according to the method with which it
            was trained on ImageNet. Default: *False*
    """
    if pretrained:
        if 'transform_input' not in kwargs:
            kwargs['transform_input'] = True
        if 'aux_logits' not in kwargs:
            kwargs['aux_logits'] = False
        if kwargs['aux_logits']:
            warnings.warn('auxiliary heads in the pretrained googlenet model are NOT pretrained, '
                          'so make sure to train them')
        original_aux_logits = kwargs['aux_logits']
        kwargs['aux_logits'] = True
        kwargs['init_weights'] = False
        model = GoogLeNet(**kwargs)
        model.load_state_dict(model_zoo.load_url(model_urls['googlenet']))
        if not original_aux_logits:
            model.aux_logits = False
            del model.aux1, model.aux2
        return model

    return GoogLeNet(**kwargs)


class GoogLeNet(nn.Module):

    def __init__(self, num_classes=1000, aux_logits=True, transform_input=False, init_weights=True):
        super(GoogLeNet, self).__init__()
        self.aux_logits = aux_logits
        self.transform_input = transform_input

        self.conv1 = BasicConv2d(3, 64, kernel_size=7, stride=2, padding=3)
        self.maxpool1 = nn.MaxPool2d(3, stride=2, ceil_mode=True)
        self.conv2 = BasicConv2d(64, 64, kernel_size=1)
        self.conv3 = BasicConv2d(64, 192, kernel_size=3, padding=1)
        self.maxpool2 = nn.MaxPool2d(3, stride=2, ceil_mode=True)

        self.inception3a = Inception(192, 64, 96, 128, 16, 32, 32)
        self.inception3b = Inception(256, 128, 128, 192, 32, 96, 64)
        self.maxpool3 = nn.MaxPool2d(3, stride=2, ceil_mode=True)

        self.inception4a = Inception(480, 192, 96, 208, 16, 48, 64)
        self.inception4b = Inception(512, 160, 112, 224, 24, 64, 64)
        self.inception4c = Inception(512, 128, 128, 256, 24, 64, 64)
        self.inception4d = Inception(512, 112, 144, 288, 32, 64, 64)
        self.inception4e = Inception(528, 256, 160, 320, 32, 128, 128)
        self.maxpool4 = nn.MaxPool2d(2, stride=2, ceil_mode=True)

        self.inception5a = Inception(832, 256, 160, 320, 32, 128, 128)
        self.inception5b = Inception(832, 384, 192, 384, 48, 128, 128)

        if aux_logits:
            self.aux1 = InceptionAux(512, num_classes)
            self.aux2 = InceptionAux(528, num_classes)

        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))
        self.dropout = nn.Dropout(0.2)
        self.fc = nn.Linear(1024, num_classes)

        if init_weights:
            self._initialize_weights()

    def _initialize_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):
                import scipy.stats as stats
                X = stats.truncnorm(-2, 2, scale=0.01)
                values = torch.as_tensor(X.rvs(m.weight.numel()), dtype=m.weight.dtype)
                values = values.view(m.weight.size())
                with torch.no_grad():
                    m.weight.copy_(values)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)

    def forward(self, x):
        if self.transform_input:
            x_ch0 = torch.unsqueeze(x[:, 0], 1) * (0.229 / 0.5) + (0.485 - 0.5) / 0.5
            x_ch1 = torch.unsqueeze(x[:, 1], 1) * (0.224 / 0.5) + (0.456 - 0.5) / 0.5
            x_ch2 = torch.unsqueeze(x[:, 2], 1) * (0.225 / 0.5) + (0.406 - 0.5) / 0.5
            x = torch.cat((x_ch0, x_ch1, x_ch2), 1)

        # N x 3 x 224 x 224
        x = self.conv1(x)
        # N x 64 x 112 x 112
        x = self.maxpool1(x)
        # N x 64 x 56 x 56
        x = self.conv2(x)
        # N x 64 x 56 x 56
        x = self.conv3(x)
        # N x 192 x 56 x 56
        x = self.maxpool2(x)

        # N x 192 x 28 x 28
        x = self.inception3a(x)
        # N x 256 x 28 x 28
        x = self.inception3b(x)
        # N x 480 x 28 x 28
        x = self.maxpool3(x)
        # N x 480 x 14 x 14
        x = self.inception4a(x)
        # N x 512 x 14 x 14
        if self.training and self.aux_logits:
            aux1 = self.aux1(x)

        x = self.inception4b(x)
        # N x 512 x 14 x 14
        x = self.inception4c(x)
        # N x 512 x 14 x 14
        x = self.inception4d(x)
        # N x 528 x 14 x 14
        if self.training and self.aux_logits:
            aux2 = self.aux2(x)

        x = self.inception4e(x)
        # N x 832 x 14 x 14
        x = self.maxpool4(x)
        # N x 832 x 7 x 7
        x = self.inception5a(x)
        # N x 832 x 7 x 7
        x = self.inception5b(x)
        # N x 1024 x 7 x 7

        #x = self.avgpool(x)
        # N x 1024 x 1 x 1
        #x = x.view(x.size(0), -1)
        # N x 1024
        #x = self.dropout(x)
        #x = self.fc(x)
        # N x 1000 (num_classes)
        if self.training and self.aux_logits:
            return _GoogLeNetOuputs(x, aux2, aux1)
        return x


class Inception(nn.Module):

    def __init__(self, in_channels, ch1x1, ch3x3red, ch3x3, ch5x5red, ch5x5, pool_proj):
        super(Inception, self).__init__()

        self.branch1 = BasicConv2d(in_channels, ch1x1, kernel_size=1)

        self.branch2 = nn.Sequential(
            BasicConv2d(in_channels, ch3x3red, kernel_size=1),
            BasicConv2d(ch3x3red, ch3x3, kernel_size=3, padding=1)
        )

        self.branch3 = nn.Sequential(
            BasicConv2d(in_channels, ch5x5red, kernel_size=1),
            BasicConv2d(ch5x5red, ch5x5, kernel_size=3, padding=1)
        )

        self.branch4 = nn.Sequential(
            nn.MaxPool2d(kernel_size=3, stride=1, padding=1, ceil_mode=True),
            BasicConv2d(in_channels, pool_proj, kernel_size=1)
        )

    def forward(self, x):
        branch1 = self.branch1(x)
        branch2 = self.branch2(x)
        branch3 = self.branch3(x)
        branch4 = self.branch4(x)

        outputs = [branch1, branch2, branch3, branch4]
        return torch.cat(outputs, 1)


class InceptionAux(nn.Module):

    def __init__(self, in_channels, num_classes):
        super(InceptionAux, self).__init__()
        self.conv = BasicConv2d(in_channels, 128, kernel_size=1)

        self.fc1 = nn.Linear(2048, 1024)
        self.fc2 = nn.Linear(1024, num_classes)

    def forward(self, x):
        # aux1: N x 512 x 14 x 14, aux2: N x 528 x 14 x 14
        x = F.adaptive_avg_pool2d(x, (4, 4))
        # aux1: N x 512 x 4 x 4, aux2: N x 528 x 4 x 4
        x = self.conv(x)
        # N x 128 x 4 x 4
        x = x.view(x.size(0), -1)
        # N x 2048
        x = F.relu(self.fc1(x), inplace=True)
        # N x 2048
        x = F.dropout(x, 0.7, training=self.training)
        # N x 2048
        x = self.fc2(x)
        # N x 1024

        return x


class BasicConv2d(nn.Module):

    def __init__(self, in_channels, out_channels, **kwargs):
        super(BasicConv2d, self).__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, bias=False, **kwargs)
        self.bn = nn.BatchNorm2d(out_channels, eps=0.001)

    def forward(self, x):
        x = self.conv(x)
        x = self.bn(x)
        return F.relu(x, inplace=True)

  
# csv file name 
filename = "ImageID.csv"
  
# initializing the titles and rows list 
fields = [] 
rows = [] 
imdata=None
model = googlenet(pretrained=True)

image=np.zeros(shape=(1,224,224,3))
image=np.rollaxis(image,3,1)
#print(torch.from_numpy(image))
#imdata[i]=model(torch.tensor(image,dtype=torch.double)).numpy()
a=torch.tensor(image,dtype=torch.float32)
#print(a)
model(a).detach().numpy().shape

